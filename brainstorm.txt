

Metrics: 
We might want to avoid using metrics because we will not be able to preprocess the final test data.
Theoretically, with self attention the Transformer can learn all kinds of relationships just from the inputs. (Feature Engineering)

    - PE Ratio                  -->     Price / Earnings (Useful for comparing companies in the same industry only)
    - Price to Book Ration      -->     Market Price per Share / Book Value per Share (Evaluates if stock is over or undervalued)

Ideas:

    - Tokenize company sectors
    - Differentiate between different types of markets (Prime, Standard, Growth) [https://www.jpx.co.jp/english/equities/market-restructure/market-segments/index.html]
    - Use data from company's quarterly earning reports as input
    - Evaluate the significance of the following files and how to implement them in the model:
        - options.csv: Data on various stock options that can help make implicit market predicitions on future stock price.
        - secondary_stock_prices.csv: Data on securities that give a better sense on how the market is performing as a whole.
        - trades.csv: Summary of previous week's trading volumes.

Questions:

    - SecuritiesCode [1300 - 10000] vs OptionsCode [130M - 199M] (Can they be of the same company?)
    - Does giving the model too many input features facilitate overfitting?
    - Which input features are more important to making predicitions?
    - Memory Management
        - How large of a pretrained model? Pros and Cons
        - Larger input size fills up memory quicker
        - FP16 [10^4] vs FP32 [10^38] (Masked Attention uses -inf + softmax to mask the top right corner of the attention matrix)
        
