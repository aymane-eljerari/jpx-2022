{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aymane/Kaggle/jpx-2022/venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import helper_functions as hf\n",
    "from dataloader import JPXData\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torchvision import transforms\n",
    "from statistics import mean\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "# pd.set_option('display.max_rows', 100)\n",
    "torch.set_printoptions(profile=\"default\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to create the new modified_data folder\n",
    "# ~ 1 min runtime\n",
    "\n",
    "# hf.concat_data()\n",
    "\n",
    "# Run this to tokenize Section/Product and NewMarketSegmentColumns on stock_list.csv\n",
    "# ~ 20 secs runtime\n",
    "\n",
    "# hf.tokenize_stock_list()\n",
    "\n",
    "# Run this to adjust the closing price\n",
    "# ~ 2 mins runtime\n",
    "\n",
    "# hf.adjust_price()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_prices = pd.read_csv(\"modified_data/autoencoder_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~ 3 month testing time frame 2022-01-04 to 2022-04-28\n",
    "train_data = stock_prices[stock_prices[\"Date\"] < '2022-01-04'].copy().reset_index(drop=True)\n",
    "test_data  = stock_prices[stock_prices[\"Date\"] >= '2022-01-04'].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aymane/Kaggle/jpx-2022/dataloader.py:100: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  data = np.array([calc_io(date) for date in self.dates], dtype=object)\n",
      "/home/aymane/Kaggle/jpx-2022/dataloader.py:100: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  data = np.array([calc_io(date) for date in self.dates], dtype=object)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'nan_to_num'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/aymane/Kaggle/jpx-2022/training.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/aymane/Kaggle/jpx-2022/training.ipynb#ch0000004?line=0'>1</a>\u001b[0m train_class \u001b[39m=\u001b[39m JPXData(data\u001b[39m=\u001b[39mtrain_data, model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmlp\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/aymane/Kaggle/jpx-2022/training.ipynb#ch0000004?line=1'>2</a>\u001b[0m trainloader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(train_class, batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, num_workers\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/aymane/Kaggle/jpx-2022/training.ipynb#ch0000004?line=2'>3</a>\u001b[0m test_class  \u001b[39m=\u001b[39m JPXData(data\u001b[39m=\u001b[39;49mtest_data, model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mae\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/aymane/Kaggle/jpx-2022/training.ipynb#ch0000004?line=3'>4</a>\u001b[0m testloader  \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(test_class, batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, num_workers\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n",
      "File \u001b[0;32m~/Kaggle/jpx-2022/dataloader.py:29\u001b[0m, in \u001b[0;36mJPXData.__init__\u001b[0;34m(self, data, model, data_dir, stock_list, stock_prices)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39m# Store mean and variance for each stock to normalize\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstock_means, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstock_stds  \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__calc_means_stds()\n\u001b[0;32m---> 29\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minputs , \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtargets \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__process_data()\n",
      "File \u001b[0;32m~/Kaggle/jpx-2022/dataloader.py:106\u001b[0m, in \u001b[0;36mJPXData.__process_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m output_tensor \u001b[39m=\u001b[39m data[:, \u001b[39m1\u001b[39m]\n\u001b[1;32m    105\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mae\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 106\u001b[0m \t\u001b[39mreturn\u001b[39;00m input_tensor\u001b[39m.\u001b[39;49mnan_to_num(nan\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), input_tensor\n\u001b[1;32m    108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmlp\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    109\u001b[0m \tinput_tensor  \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([np\u001b[39m.\u001b[39mhstack(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m input_tensor])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'nan_to_num'"
     ]
    }
   ],
   "source": [
    "train_class = JPXData(data=train_data, model=\"mlp\")\n",
    "trainloader = torch.utils.data.DataLoader(train_class, batch_size=1, shuffle=True, num_workers=5)\n",
    "test_class  = JPXData(data=test_data, model=\"ae\")\n",
    "testloader  = torch.utils.data.DataLoader(test_class, batch_size=1, shuffle=False, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 69])\n"
     ]
    }
   ],
   "source": [
    "a = test_class[0]\n",
    "print(a[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1213\n"
     ]
    }
   ],
   "source": [
    "a = '12_13'\n",
    "print(int(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking Securities Codes by rate of change from t+1 to t+2\n",
    "stock_prices = pd.read_csv(\"modified_data/stock_prices.csv\")\n",
    "date = \"2017-01-04\"\n",
    "targets = stock_prices.query(\"Date == @date\")[[\"Target\", \"SecuritiesCode\"]].fillna(0)\n",
    "df = targets.sort_values(by=\"Target\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train_class\n",
    "print(len(a[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Verifying if the ranking is the same\n",
    "# securities_codes    = stock_prices[\"SecuritiesCode\"]\n",
    "# securities_list     = sorted(list(set(securities_codes)))\n",
    "# securities_dict     = {i: securities_list.index(i) for i in securities_list}\n",
    "\n",
    "# a = dataloader[0][1]\n",
    "# sorted_args = torch.argsort(a, descending=True)\n",
    "# securities_sort = [list(securities_dict.keys())[list(securities_dict.values()).index(i)] for i in sorted_args]\n",
    "# print(securities_sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "69*2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Linear(138000, 7000),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.Linear(7000, 5000),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.Linear(5000, 2000),\n",
    "            # nn.LeakyReLU(negative_slope=0.1),\n",
    "            # nn.Linear(1000, 1500),\n",
    "            # nn.LeakyReLU(negative_slope=0.1),\n",
    "            # nn.Linear(1500, 2000),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.layer_stack(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss().cuda()\n",
    "optimizer = optim.SGD(net.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_a1 = []\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels, date = data\n",
    "        inputs\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = hf.loss_function(outputs * 10, labels * 10, criterion=nn.HuberLoss(delta=5))\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(net.parameters(), 1.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        loss_a1.append(running_loss)\n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.10f}')\n",
    "            running_loss = 0.0\n",
    "        # torch.save(net.state_dict(), f'saved_models/{epoch}.pt')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_a1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame()\n",
    "dates = []\n",
    "securities = []\n",
    "preds = []\n",
    "targets = []\n",
    "\n",
    "stock_ids = test_class.stock_id.tolist()\n",
    "\n",
    "for X, target, date in tqdm(test_class):\n",
    "    X = X.cuda()\n",
    "    pred = net(X)\n",
    "\n",
    "    target_list = target.tolist()\n",
    "\n",
    "    dates.append(date*2000)\n",
    "    securities.append(stock_ids)\n",
    "    preds.append(pred.tolist())\n",
    "    targets.append(target_list)\n",
    "\n",
    "    new_dates = list(itertools.chain.from_iterable(dates))\n",
    "    new_securities = list(itertools.chain.from_iterable(securities))\n",
    "    new_preds = list(itertools.chain.from_iterable(preds))\n",
    "    new_targets = list(itertools.chain.from_iterable(targets))\n",
    "\n",
    "pred_df[\"Date\"] = new_dates\n",
    "pred_df[\"SecuritiesCode\"] = new_securities\n",
    "pred_df[\"Prediction\"] = new_preds\n",
    "pred_df[\"Target\"] = new_targets\n",
    "print(pred_df)\n",
    "# predictions = pd.DataFrame(list(zip(dates, securitiescodes, targets)), columns=[\"Date\", \"SecuritiesCode\", \"Target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pred_df.groupby(\"Date\")\n",
    "\n",
    "securities_dict = {i: stock_ids.index(i) for i in stock_ids}\n",
    "rank = []\n",
    "for i in tqdm(dates):\n",
    "    sorted_args = torch.argsort(torch.tensor(list(i[1][\"Prediction\"])), descending=True)\n",
    "    securities_sort = [list(securities_dict.keys())[list(securities_dict.values()).index(i)] for i in sorted_args]\n",
    "    sorted_ranks = [securities_sort.index(i) for i in stock_ids]\n",
    "    rank.append(sorted_ranks)\n",
    "\n",
    "new_rank = list(itertools.chain.from_iterable(rank))\n",
    "pred_df[\"Rank\"] = new_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.calc_spread_return_sharpe(pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e2db06da75207f0b335e9532cc533497eb90d83ed622a6d9d355f2997ed09f04"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
