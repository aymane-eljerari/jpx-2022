{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aymane/Kaggle/jpx-2022/venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from dataloader import JPXData\n",
    "from dataloader2 import SecondData\n",
    "from autoencoder import encoder\n",
    "import helper_functions as hf\n",
    "import matplotlib.pyplot as plt\n",
    "from autoencoder import autoencoder as ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_prices = pd.read_csv(\"modified_data/autoencoder_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class = JPXData(data=stock_prices, model=\"ae\")\n",
    "trainloader = torch.utils.data.DataLoader(train_class, batch_size=1, shuffle=True, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [69, 50, 35, 20, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ae(e_layers=layers, d_layers=layers[::-1]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss().cuda()\n",
    "optimizer = optim.SGD(net.parameters(), lr=1e-4, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  1000] loss: 43.66825 L1_loss: 0.45330\n",
      "[2,  1000] loss: 27.24378 L1_loss: 0.34740\n",
      "[3,  1000] loss: 24.41209 L1_loss: 0.32623\n",
      "[4,  1000] loss: 22.33589 L1_loss: 0.31318\n",
      "[5,  1000] loss: 20.04292 L1_loss: 0.29311\n",
      "[6,  1000] loss: 19.54267 L1_loss: 0.28878\n",
      "[7,  1000] loss: 19.11541 L1_loss: 0.28586\n",
      "[8,  1000] loss: 18.60277 L1_loss: 0.28057\n",
      "[9,  1000] loss: 18.42588 L1_loss: 0.27860\n",
      "[10,  1000] loss: 18.03478 L1_loss: 0.27602\n",
      "[11,  1000] loss: 17.83141 L1_loss: 0.27351\n",
      "[12,  1000] loss: 17.44952 L1_loss: 0.27128\n",
      "[13,  1000] loss: 17.10300 L1_loss: 0.26806\n",
      "[14,  1000] loss: 16.84014 L1_loss: 0.26620\n",
      "[15,  1000] loss: 16.84382 L1_loss: 0.26604\n",
      "[16,  1000] loss: 16.71111 L1_loss: 0.26470\n",
      "[17,  1000] loss: 16.55539 L1_loss: 0.26386\n",
      "[18,  1000] loss: 16.25094 L1_loss: 0.26168\n",
      "[19,  1000] loss: 16.19532 L1_loss: 0.26108\n",
      "[20,  1000] loss: 15.85309 L1_loss: 0.25845\n",
      "[21,  1000] loss: 16.03016 L1_loss: 0.25968\n",
      "[22,  1000] loss: 15.78403 L1_loss: 0.25800\n",
      "[23,  1000] loss: 15.53589 L1_loss: 0.25632\n",
      "[24,  1000] loss: 15.61098 L1_loss: 0.25839\n",
      "[25,  1000] loss: 15.29565 L1_loss: 0.25428\n",
      "[26,  1000] loss: 15.47270 L1_loss: 0.25730\n",
      "[27,  1000] loss: 15.13695 L1_loss: 0.25348\n",
      "[28,  1000] loss: 15.33774 L1_loss: 0.25573\n",
      "[29,  1000] loss: 15.16238 L1_loss: 0.25460\n",
      "[30,  1000] loss: 14.96739 L1_loss: 0.25263\n",
      "[31,  1000] loss: 15.19455 L1_loss: 0.25601\n",
      "[32,  1000] loss: 14.82114 L1_loss: 0.25062\n",
      "[33,  1000] loss: 14.82641 L1_loss: 0.25180\n",
      "[34,  1000] loss: 14.78951 L1_loss: 0.25104\n",
      "[35,  1000] loss: 14.85954 L1_loss: 0.25220\n",
      "[36,  1000] loss: 14.73578 L1_loss: 0.25205\n",
      "[37,  1000] loss: 14.89669 L1_loss: 0.25351\n",
      "[38,  1000] loss: 14.55884 L1_loss: 0.24884\n",
      "[39,  1000] loss: 14.73067 L1_loss: 0.25196\n",
      "[40,  1000] loss: 14.52948 L1_loss: 0.24851\n",
      "[41,  1000] loss: 14.42707 L1_loss: 0.24797\n",
      "[42,  1000] loss: 14.44452 L1_loss: 0.24874\n",
      "[43,  1000] loss: 14.34957 L1_loss: 0.24799\n",
      "[44,  1000] loss: 14.47143 L1_loss: 0.24996\n",
      "[45,  1000] loss: 14.12602 L1_loss: 0.24547\n",
      "[46,  1000] loss: 14.30381 L1_loss: 0.24786\n",
      "[47,  1000] loss: 14.20332 L1_loss: 0.24672\n",
      "[48,  1000] loss: 14.15323 L1_loss: 0.24693\n",
      "[49,  1000] loss: 14.30421 L1_loss: 0.24783\n",
      "[50,  1000] loss: 14.17578 L1_loss: 0.24770\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "loss_a1 = []\n",
    "loss_L1 = []\n",
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_loss_l1 = 0.0\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels, date = data\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = hf.loss_function(outputs, labels, criterion=lambda x,y: criterion(10*x, 10*y))\n",
    "        loss_l1 = hf.loss_function(outputs, labels, criterion=nn.L1Loss().cuda())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        running_loss_l1 += loss_l1.item()\n",
    "    \n",
    "        if i % 1000 == 999: # print every 1000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 1000:.5f} L1_loss: {running_loss_l1 / 1000:.5f}')\n",
    "            loss_a1.append(running_loss /1000)\n",
    "            loss_L1.append(running_loss_l1/1000)\n",
    "            running_loss = 0.0\n",
    "            running_loss_l1 = 0.0\n",
    "        # torch.save(net.state_dict(), f'saved_models/{epoch}.pt')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_loss = np.array(loss_L1)\n",
    "plt.plot(final_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = final_loss[-1]\n",
    "fname = f\"autoencoder_saved/ae1_l1_{name:0.3f}.pt\"\n",
    "net.save_state(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encoder(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=69, out_features=50, bias=True)\n",
       "    (1): SELU()\n",
       "    (2): Linear(in_features=50, out_features=35, bias=True)\n",
       "    (3): SELU()\n",
       "    (4): Linear(in_features=35, out_features=20, bias=True)\n",
       "    (5): SELU()\n",
       "    (6): Linear(in_features=20, out_features=10, bias=True)\n",
       "    (7): SELU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = encoder(layers) \n",
    "e.load_state_dict(torch.load(fname))\n",
    "e.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ae2 = SecondData(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20000])\n"
     ]
    }
   ],
   "source": [
    "a = data_ae2[0]\n",
    "print(a[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [20000, 5000]\n",
    "net2 = ae(e_layers=layers, d_layers=layers[::-1]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "for i in range(len(layers)-1):\n",
    "    sum += layers[i] * layers[i+1]\n",
    "print(sum*8/1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss().cuda()\n",
    "optimizer = optim.SGD(net2.parameters(), lr=1e-4, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/aymane/Kaggle/jpx-2022/train.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/aymane/Kaggle/jpx-2022/train.ipynb#ch0000013?line=17'>18</a>\u001b[0m loss \u001b[39m=\u001b[39m hf\u001b[39m.\u001b[39mloss_function(outputs, labels, criterion\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x,y: criterion(\u001b[39m10\u001b[39m\u001b[39m*\u001b[39mx, \u001b[39m10\u001b[39m\u001b[39m*\u001b[39my))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/aymane/Kaggle/jpx-2022/train.ipynb#ch0000013?line=18'>19</a>\u001b[0m loss_L1_2 \u001b[39m=\u001b[39m hf\u001b[39m.\u001b[39mloss_function(outputs, labels, criterion\u001b[39m=\u001b[39mnn\u001b[39m.\u001b[39mL1Loss()\u001b[39m.\u001b[39mcuda())\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/aymane/Kaggle/jpx-2022/train.ipynb#ch0000013?line=19'>20</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/aymane/Kaggle/jpx-2022/train.ipynb#ch0000013?line=20'>21</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/aymane/Kaggle/jpx-2022/train.ipynb#ch0000013?line=22'>23</a>\u001b[0m \u001b[39m# print statistics\u001b[39;00m\n",
      "File \u001b[0;32m~/Kaggle/jpx-2022/venv/lib/python3.10/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/Kaggle/jpx-2022/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "loss_a2 = []\n",
    "loss_L1_2 = []\n",
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_loss_L1_2 = 0.0\n",
    "\n",
    "    for i, data in enumerate(data_ae2, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = net2(inputs)\n",
    "        loss = hf.loss_function(outputs, labels, criterion=lambda x,y: criterion(10*x, 10*y))\n",
    "        loss_L1_2 = hf.loss_function(outputs, labels, criterion=nn.L1Loss().cuda())\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        running_loss_L1_2 += loss_L1_2.item()\n",
    "    \n",
    "        if i % 1000 == 999: # print every 1000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 1000:.5f} L1_loss: {running_loss_L1_2 / 1000:.5f}')\n",
    "            loss_a2.append(running_loss /1000)\n",
    "            loss_L1_2.append(running_loss_L1_2/1000)\n",
    "            running_loss = 0.0\n",
    "            running_loss_L1_2 = 0.0\n",
    "        # torch.save(net.state_dict(), f'saved_models/{epoch}.pt')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e2db06da75207f0b335e9532cc533497eb90d83ed622a6d9d355f2997ed09f04"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
